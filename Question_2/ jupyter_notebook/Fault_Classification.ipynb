{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashishbhatta/Assignment-2/blob/main/Question_2/%20jupyter_notebook/Fault_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import Required Packages**"
      ],
      "metadata": {
        "id": "tKkdW4p8QXF1"
      },
      "id": "tKkdW4p8QXF1"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "a17e6ac4",
      "metadata": {
        "id": "a17e6ac4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "import time\n",
        "\n",
        "from sklearn.model_selection import train_test_split #needed for spliting data into testing and training\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import svm\n",
        "from sklearn import tree\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "#from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import GridSearchCV, RepeatedKFold"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Defining ML Classifier Models to be used**"
      ],
      "metadata": {
        "id": "Le0fIs1BN-qN"
      },
      "id": "Le0fIs1BN-qN"
    },
    {
      "cell_type": "code",
      "source": [
        "models = {\n",
        "    'LR': LogisticRegression(),\n",
        "    'SVM': svm.SVC(),\n",
        "    'DT': tree.DecisionTreeClassifier(),\n",
        "    'ANN':  MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, random_state=42),\n",
        "    'RF': RandomForestClassifier(),\n",
        "    'XGB': XGBClassifier(n_estimators=100, random_state=42)\n",
        "}"
      ],
      "metadata": {
        "id": "HqKFRBayNS5P"
      },
      "id": "HqKFRBayNS5P",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Read Data**"
      ],
      "metadata": {
        "id": "zfaFdmHuN5pX"
      },
      "id": "zfaFdmHuN5pX"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "36605e0b",
      "metadata": {
        "id": "36605e0b"
      },
      "outputs": [],
      "source": [
        "def read_data():\n",
        "  df_LG = pd.read_csv(\"https://github.com/ashishbhatta/Assignment-2/raw/refs/heads/main/Question_2/Datas/LG.csv\")\n",
        "  df_LL = pd.read_csv(\"https://github.com/ashishbhatta/Assignment-2/raw/refs/heads/main/Question_2/Datas/LL.csv\")\n",
        "  df_LLG=pd.read_csv(\"https://github.com/ashishbhatta/Assignment-2/raw/refs/heads/main/Question_2/Datas/LLG.csv\")\n",
        "  df_LLL = pd.read_csv(\"https://github.com/ashishbhatta/Assignment-2/raw/refs/heads/main/Question_2/Datas/LLL_fault.csv\")\n",
        "\n",
        "  df = pd.concat([df_LG, df_LL, df_LLG, df_LLL],ignore_index=True,axis=0) # ignore_index=True,\n",
        "  print(df)\n",
        "\n",
        "  data=np.array(df)\n",
        "  X=data[:,1:4]\n",
        "  y=data[:,4]\n",
        "  return(X,y)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Visualization**"
      ],
      "metadata": {
        "id": "xsXLg9KMN2d7"
      },
      "id": "xsXLg9KMN2d7"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "1663607e",
      "metadata": {
        "id": "1663607e"
      },
      "outputs": [],
      "source": [
        "#Step 1: Understanding the Data\n",
        "def visualize_data(X,y):\n",
        "  # Scatter plot of the inputs vs target\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  plt.scatter(X[:, 0], y, label='I_a vs Fault Type')\n",
        "  plt.scatter(X[:, 1], y, label='I_b vs Fault Type')\n",
        "  plt.scatter(X[:, 2], y, label='I_c vs Fault Type')\n",
        "\n",
        "  plt.xlabel('Fault Current in kA')\n",
        "  plt.ylabel('Fault Type')\n",
        "  plt.legend()\n",
        "  plt.title('Output vs Input Visualization')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Splitting the Data**"
      ],
      "metadata": {
        "id": "axW_OedSNzoa"
      },
      "id": "axW_OedSNzoa"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a8ee3d29",
      "metadata": {
        "id": "a8ee3d29"
      },
      "outputs": [],
      "source": [
        "#Step2: Scaling the data\n",
        "def split_data(X,y,test_size):\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=42,test_size=test_size)\n",
        "  #print(\"Train data are:\")\n",
        "  print(X_test.size)\n",
        "  print(y_test.size)\n",
        "\n",
        "  print(X_train.size)\n",
        "  print(y_train.size)\n",
        "  return(X_train, X_test, y_train, y_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Normalizing the inputs**"
      ],
      "metadata": {
        "id": "RH6ZrlkZNw_u"
      },
      "id": "RH6ZrlkZNw_u"
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_inputs(X_train,X_test):\n",
        "    #Normalizing the Input\n",
        "  scaler = StandardScaler()\n",
        "  scaler.fit(X_train)\n",
        "  X_train_scaled = scaler.transform(X_train)\n",
        "  X_test_scaled = scaler.transform(X_test)\n",
        "  return(X_train_scaled,X_test_scaled)"
      ],
      "metadata": {
        "id": "Dt3Ldy7ZLOe6"
      },
      "id": "Dt3Ldy7ZLOe6",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Function for training, testing the model**"
      ],
      "metadata": {
        "id": "h4LSL8-tNlu6"
      },
      "id": "h4LSL8-tNlu6"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "e2fa0913",
      "metadata": {
        "id": "e2fa0913"
      },
      "outputs": [],
      "source": [
        "def ML_classifier(classifier,train_inputs,train_target, test_inputs, test_output,model_name = \"\"):\n",
        "\n",
        "    result = {\n",
        "        'Model': model_name,\n",
        "        'Database Generation Time (min)': np.nan,  # Placeholder\n",
        "        'Training Accuracy': np.nan,\n",
        "        'Training Precision': np.nan,\n",
        "        'Training Recall': np.nan,\n",
        "        'Training F1-Score': np.nan,\n",
        "        'Training Time (min)': np.nan,\n",
        "        'Testing Accuracy': np.nan,\n",
        "        'Testing Precision': np.nan,\n",
        "        'Testing Recall': np.nan,\n",
        "        'Testing F1-Score': np.nan,\n",
        "        'Testing Time (min)': np.nan\n",
        "    }\n",
        "\n",
        "    db_gen_time = 0.1  # Set your actual database generation time in minutes\n",
        "    result['Database Generation Time (min)'] = db_gen_time\n",
        "\n",
        "    t = time.process_time()\n",
        "    classifier.fit(train_inputs,train_target)\n",
        "    training_time = time.process_time() - t\n",
        "\n",
        "    result['Training Accuracy'] = accuracy_score(train_target, classifier.predict(train_inputs))\n",
        "    result['Training Precision'] = precision_score(train_target, classifier.predict(train_inputs), average='micro')\n",
        "    result['Training Recall'] = recall_score(train_target, classifier.predict(train_inputs), average='micro')\n",
        "    result['Training F1-Score'] = f1_score(train_target, classifier.predict(train_inputs), average='micro')\n",
        "    result['Training Time (min)'] = training_time\n",
        "\n",
        "\n",
        "    t = time.process_time()\n",
        "    classifier.fit(test_inputs, test_output) #for MLR\n",
        "    testing_time = time.process_time() - t\n",
        "\n",
        "    y_pred=classifier.predict(test_inputs);\n",
        "    y_pred=np.array(y_pred)\n",
        "\n",
        "\n",
        "    result['Testing Accuracy'] = accuracy_score(test_output, y_pred)\n",
        "    result['Testing Precision'] = precision_score(test_output, y_pred, average='micro')\n",
        "    result['Testing Recall'] = recall_score(test_output, y_pred, average='micro')\n",
        "    result['Testing F1-Score'] = f1_score(test_output, y_pred, average='micro')\n",
        "    result['Testing Time (min)'] = testing_time\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    cm = confusion_matrix(test_output, y_pred)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(test_output), yticklabels=np.unique(test_output))\n",
        "    plt.xlabel('Predicted Labels')\n",
        "    plt.ylabel('True Labels')\n",
        "    plt.title(f'Confusion Matrix Heatmap for {model_name}')\n",
        "    plt.savefig(f\"{model_name}_confusion_matrix.png\", dpi=400)\n",
        "    plt.show()\n",
        "\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hyper Parameter Tuning**"
      ],
      "metadata": {
        "id": "ieQbLT8hOoPI"
      },
      "id": "ieQbLT8hOoPI"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "7e60022e",
      "metadata": {
        "id": "7e60022e"
      },
      "outputs": [],
      "source": [
        "#Hyperparameter Tuning for ANN\n",
        "def hyperparameter_tuning(X_train_scaled,X_test_scaled, y_train,y_test):\n",
        "  param_grid = {\n",
        "      'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 100)],  # Different layer configurations\n",
        "      'activation': ['relu', 'tanh', 'logistic'],  # Activation functions\n",
        "      #'solver': ['adam', 'sgd'],  # Solvers\n",
        "      #'learning_rate': ['constant', 'invscaling', 'adaptive'],  # Learning rate schedules\n",
        "      'learning_rate_init': [0.001, 0.01, 0.1]  # Initial learning rate values\n",
        "  }\n",
        "\n",
        "  # Instantiate the classifier\n",
        "  classifier = MLPClassifier(random_state=12345, max_iter=1000)\n",
        "\n",
        "\n",
        "  # Instantiate 5-fold CV repeated 3 times (reduce folds and repeats)\n",
        "  cv = RepeatedKFold(n_splits=5, n_repeats=3, random_state=54321)\n",
        "\n",
        "\n",
        "  # Instantiate grid search CV\n",
        "  grid_cv = GridSearchCV(classifier, param_grid, scoring='accuracy', n_jobs=5, cv=cv, refit=True)\n",
        "\n",
        "  # Perform grid search\n",
        "  grid_cv.fit(X_train_scaled, y_train)\n",
        "\n",
        "  # Which parameters were the best?\n",
        "  print(f\"Best Parameters: {grid_cv.best_params_}\")\n",
        "\n",
        "  # Get the best estimator (classifier with best hyperparameters)\n",
        "  best_classifier = grid_cv.best_estimator_\n",
        "\n",
        "  # Evaluate the best model on the test set\n",
        "  y_pred = best_classifier.predict(X_test_scaled)\n",
        "  print(f\"Accuracy on Test Set: {accuracy_score(y_test, y_pred)}\")\n",
        "  return"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main Body"
      ],
      "metadata": {
        "id": "MCVy2X9ANbfc"
      },
      "id": "MCVy2X9ANbfc"
    },
    {
      "cell_type": "code",
      "source": [
        "X,y = read_data()\n",
        "visualize_data(X,y)\n",
        "X_train, X_test, y_train, y_test = split_data(X,y,0.2)\n",
        "X_train_scaled,X_test_scaled = normalize_inputs(X_train,X_test)\n",
        "results_list = []\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    result = ML_classifier(model, X_train_scaled, y_train, X_test_scaled, y_test, model_name)\n",
        "    results_list.append(result)\n",
        "    summary_df = pd.DataFrame(results_list)\n",
        "    summary_df.to_csv(\"summary.csv\")\n",
        "\n",
        "#Takes long time for hyperparameter Tuning\n",
        "#hyperparameter_tuning(X_train_scaled,X_test_scaled, y_train,y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ghDsAQt_eXm",
        "outputId": "4c948958-c970-4105-e439-f5fd6d3cd09f"
      },
      "id": "6ghDsAQt_eXm",
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      test_data       I_a       I_b       I_c  Fault_Type\n",
            "0             1  2.789859  0.000000  0.000000         2.0\n",
            "1             2  2.794565  0.000000  0.000000         2.0\n",
            "2             3  2.795167  0.000000  0.000000         2.0\n",
            "3             4  2.795763  0.000000  0.000000         2.0\n",
            "4             5  2.788781  0.000000  0.000000         2.0\n",
            "...         ...       ...       ...       ...         ...\n",
            "3995        996  2.650179  2.650179  2.650179         0.0\n",
            "3996        997  2.649544  2.649544  2.649544         0.0\n",
            "3997        998  2.645722  2.645722  2.645722         0.0\n",
            "3998        999  2.643605  2.643605  2.643605         0.0\n",
            "3999       1000  2.649951  2.649951  2.649951         0.0\n",
            "\n",
            "[4000 rows x 5 columns]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/seaborn/categorical.py:3399: UserWarning: 99.8% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n",
            "  warnings.warn(msg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/seaborn/categorical.py:3399: UserWarning: 50.0% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n",
            "  warnings.warn(msg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/seaborn/categorical.py:3399: UserWarning: 75.0% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n",
            "  warnings.warn(msg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/seaborn/categorical.py:3399: UserWarning: 66.7% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n",
            "  warnings.warn(msg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/seaborn/categorical.py:3399: UserWarning: 99.8% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n",
            "  warnings.warn(msg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/seaborn/categorical.py:3399: UserWarning: 50.0% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n",
            "  warnings.warn(msg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/seaborn/categorical.py:3399: UserWarning: 75.0% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n",
            "  warnings.warn(msg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/seaborn/categorical.py:3399: UserWarning: 66.7% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n",
            "  warnings.warn(msg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/seaborn/categorical.py:3399: UserWarning: 99.8% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n",
            "  warnings.warn(msg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/seaborn/categorical.py:3399: UserWarning: 50.0% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n",
            "  warnings.warn(msg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/seaborn/categorical.py:3399: UserWarning: 75.0% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n",
            "  warnings.warn(msg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/seaborn/categorical.py:3399: UserWarning: 66.7% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n",
            "  warnings.warn(msg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}